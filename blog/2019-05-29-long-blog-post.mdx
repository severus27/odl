---
slug: peft-lora
title: In-depth Fine Tuning PEFT with LoRA & QLoRA
authors: endi
tags: [hello, docusaurus]
---

Fine-tuning is a crucial step in unlocking the full potential of large language models (LLMs). It involves adapting the pre-trained LLM to a specific task or domain, resulting in significantly improved performance on that task. However, complete fine-tuning can be computationally expensive and memory-intensive, limiting their applicability on resource-constrained platforms.

To address this challenge, researchers have developed [parameter-efficient fine-tuning](https://huggingface.co/docs/peft/index) (PEFT) techniques. These techniques aim to achieve similar or even better performance than traditional fine-tuning while significantly reducing the number of parameters and memory footprint. Among the most promising PEFT approaches are [LoRA](https://huggingface.co/docs/peft/conceptual_guides/lora) (Low-Rank Adaptation) and [QLoRA](https://huggingface.co/blog/4bit-transformers-bitsandbytes) (Quantized LoRA).

In neural networks, the weight matrices consist of [floating-point numbers](https://en.wikipedia.org/wiki/Floating-point_arithmetic), typically stored in the 32-bit floating-point data type. Computers internally represent these floating-point values using binary code, achieved by flipping bits for the [sign](https://en.wikipedia.org/wiki/Integer_(computer_science)), [exponent](https://en.wikipedia.org/wiki/Exponentiation), and [significant](https://en.wikipedia.org/wiki/Significand) (mantissa). This binary representation is fundamental to accurately handle and process numerical data within the neural network architecture.

To reduce precision, a common approach is adjusting the data types. One option is shifting to [half precision](https://en.wikipedia.org/wiki/Half-precision_floating-point_format) (16-bit), utilizing only half the bits for number representation, cutting memory needs in half. However, there's a trade-off, where precision is sacrificed.

